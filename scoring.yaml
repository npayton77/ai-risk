scoring:
  # Scoring matrices for each dimension (1-4 scale)
  dimensions:
    autonomy:
      tool: 1
      assistant: 2
      agent: 3
      autonomous: 4
    
    oversight:
      continuous: 1
      checkpoint: 2
      exception: 3
      minimal: 4
    
    impact:
      informational: 1
      operational: 2
      strategic: 3
      external: 4
    
    orchestration:
      single: 1
      sequential: 2
      parallel: 3
      hierarchical: 4
    
    data_sensitivity:
      public: 1
      internal: 2
      confidential: 3
      regulated: 4

  # Risk level thresholds based on total score (5-20 range with new data sensitivity dimension)
  risk_thresholds:
    - min_score: 5
      max_score: 8
      level: "low"
    - min_score: 9
      max_score: 13
      level: "medium"
    - min_score: 14
      max_score: 17
      level: "high"
    - min_score: 18
      max_score: 20
      level: "critical"

recommendations:
  # Base recommendations by risk level
  by_risk_level:
    low:
      - "Implement standard review processes for AI outputs"
      - "Establish clear escalation paths for edge cases"
      - "Maintain documentation of AI decisions and human overrides"
    
    medium:
      - "Implement regular audits of AI decision quality"
      - "Set up confidence thresholds for autonomous actions"
      - "Establish fallback procedures for AI failures"
      - "Monitor for model drift and performance degradation"
    
    high:
      - "Implement comprehensive real-time monitoring"
      - "Set up automatic circuit breakers for anomalous behavior"
      - "Develop rapid rollback capabilities"
      - "Assign dedicated AI oversight personnel"
      - "Conduct regular stress testing"
    
    critical:
      - "Implement sandboxed testing environments"
      - "Require formal verification of AI behavior"
      - "Install emergency kill switches"
      - "Conduct extensive simulation testing before deployment"
      - "Consider if this risk level is acceptable for your organization"

  # Conditional recommendations based on specific combinations
  conditional:
    - condition:
        autonomy: ["agent", "autonomous"]
        oversight: ["minimal"]
      recommendation: "HIGH PRIORITY: Increase human oversight given high autonomy level"
    
    - condition:
        impact: ["external"]
        oversight: ["exception", "minimal"]
      recommendation: "CRITICAL: External impact requires more frequent human review"
    
    - condition:
        autonomy: ["autonomous"]
      recommendation: "Consider implementing AI explainability tools"
    
    - condition:
        data_sensitivity: ["regulated"]
        autonomy: ["agent", "autonomous"]
      recommendation: "COMPLIANCE ALERT: Regulated data with high autonomy requires legal review and audit trails"
    
    - condition:
        data_sensitivity: ["confidential", "regulated"]
        oversight: ["minimal", "exception"]
      recommendation: "SECURITY PRIORITY: Sensitive data requires enhanced monitoring and access controls"

# Display styling for different risk levels
risk_styling:
  low:
    color: "#27ae60"
    bg: "#e6f3e6"
    border: "#27ae60"
    emoji: "LOW"
  
  medium:
    color: "#f39c12"
    bg: "#fff3cd"
    border: "#f39c12"
    emoji: "MED"
  
  high:
    color: "#e74c3c"
    bg: "#f8d7da"
    border: "#e74c3c"
    emoji: "HIGH"
  
  critical:
    color: "#8e44ad"
    bg: "#e2e3f3"
    border: "#8e44ad"
    emoji: "CRIT" 